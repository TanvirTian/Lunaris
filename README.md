
# ğŸŒ™ Lunaris

**Privacy analysis engine for the modern web.**

Lunaris scans any public URL and surfaces trackers, cookies, fingerprinting vectors, third-party data flows, and dark patterns â€” processed asynchronously through a production-grade queue architecture.

[![Node.js](https://img.shields.io/badge/Node.js-20-339933?style=flat-square&logo=node.js&logoColor=white)](https://nodejs.org)
[![Fastify](https://img.shields.io/badge/Fastify-4-000000?style=flat-square&logo=fastify&logoColor=white)](https://fastify.dev)
[![PostgreSQL](https://img.shields.io/badge/PostgreSQL-16-4169E1?style=flat-square&logo=postgresql&logoColor=white)](https://postgresql.org)
[![Prisma](https://img.shields.io/badge/Prisma-ORM-2D3748?style=flat-square&logo=prisma&logoColor=white)](https://prisma.io)
[![Redis](https://img.shields.io/badge/Redis-7-DC382D?style=flat-square&logo=redis&logoColor=white)](https://redis.io)
[![React](https://img.shields.io/badge/React-18-61DAFB?style=flat-square&logo=react&logoColor=black)](https://react.dev)


## What Lunaris Does

Submit any URL. Lunaris launches a headless Chromium instance, crawls the site across multiple pages, and produces a detailed privacy report:

- **Tracker detection** â€” identifies known tracking scripts, pixels, and third-party domains
- **Cookie analysis** â€” classifies cookies by purpose, lifetime, and security attributes
- **Fingerprinting detection** â€” detects canvas, WebGL, and font fingerprinting attempts
- **Ownership graph** â€” maps tracker domains back to parent corporations
- **Dark pattern signals** â€” surfaces consent manipulation and deceptive UI patterns
- **Privacy score** â€” 0â€“100 score with per-signal deductions and risk classification

Scans are processed asynchronously. The API returns a job ID immediately and the client polls for results â€” no HTTP timeouts, no blocking.


## Architecture

```
User submits URL
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚        API Layer            â”‚  â† URL validation, SSRF protection, DNS resolution
â”‚        Fastify              â”‚    deduplication check, rate limiting
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚ job created (PENDING) written to PostgreSQL
             â”‚ enqueued to BullMQ
             â”‚ jobId returned immediately (HTTP 202)
             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚       Queue Layer           â”‚  â† BullMQ + Redis
â”‚                             â”‚    persistent job storage, retry logic (3x backoff)
â”‚                             â”‚    concurrency control, dead letter queue
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚ worker picks up job
             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚      Worker Layer           â”‚  â† job marked RUNNING
â”‚                             â”‚    Playwright launches headless Chromium
â”‚   Crawl Engine              â”‚    crawls homepage + up to 3 sub-pages
â”‚   Playwright                â”‚    intercepts all network requests
â”‚        â”‚                    â”‚    extracts cookies, scripts, storage
â”‚        â–¼                    â”‚
â”‚   Analysis Pipeline         â”‚    tracker detection
â”‚                             â”‚    cookie classification
â”‚   analyzer.js               â”‚    fingerprinting detection (canvas, WebGL, font)
â”‚   cookieAnalysis.js         â”‚    script intelligence + obfuscation detection
â”‚   scriptIntelligence.js     â”‚    ownership graph (domain to corporation)
â”‚   ownershipGraph.js         â”‚    dark pattern signals
â”‚        â”‚                    â”‚
â”‚        â–¼ privacy score 0-100â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚ atomic transaction
             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     Persistence Layer       â”‚  â† ScanJob updated to SUCCESS
â”‚     PostgreSQL + Prisma     â”‚    ScanResult created with typed columns + rawData JSONB
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
             â–¼
Client polls GET /scan/:id and receives full result

```

**Key design decisions:**

-   Backend never touches Playwright â€” returns in <100ms regardless of crawl time
-   nginx proxies all API calls internally â€” browser stays on one origin, no CORS needed
-   Backend and worker share one Docker image, run different commands
-   DNS pre-resolution + private IP blocking before any browser is launched
-   Atomic DB transactions â€” no SUCCESS job without a result, no orphaned records
-   Database migrations run automatically on every backend container startup


## Tech Stack

| Layer | Technology | Purpose |
|---|---|---|
| API Server | Fastify 4 | HTTP layer, rate limiting, schema validation |
| ORM | Prisma 5 | Type-safe PostgreSQL access, migrations |
| Database | PostgreSQL 16 | Permanent storage, JSONB result blobs |
| Queue | BullMQ + Redis 7 | Async job processing, retries, DLQ |
| Crawler | Playwright + Chromium | Headless browser, fingerprint detection |
| Frontend | React 18 + Vite | UI, result polling |


## Project Structure
```
.  
â”œâ”€â”€ backend  
â”‚ â”œâ”€â”€ DB_SETUP.md  
â”‚ â”œâ”€â”€ Dockerfile  
â”‚ â”œâ”€â”€ lib  
â”‚ â”‚ â”œâ”€â”€ db.js  
â”‚ â”‚ â”œâ”€â”€ logger.js  
â”‚ â”‚ â”œâ”€â”€ metrics.js  
â”‚ â”‚ â”œâ”€â”€ queue.js  
â”‚ â”‚ â””â”€â”€ redis.js  
â”‚ â”œâ”€â”€ package.json  
â”‚ â”œâ”€â”€ package-lock.json  
â”‚ â”œâ”€â”€ prisma  
â”‚ â”‚ â”œâ”€â”€ migrations  
â”‚ â”‚ â”‚ â”œâ”€â”€ 20260223184806_init  
â”‚ â”‚ â”‚ â”‚ â””â”€â”€ migration.sql  
â”‚ â”‚ â”‚ â””â”€â”€ migration_lock.toml  
â”‚ â”‚ â””â”€â”€ schema.prisma  
â”‚ â”œâ”€â”€ routes  
â”‚ â”‚ â”œâ”€â”€ analyze.js  
â”‚ â”‚ â”œâ”€â”€ health.js  
â”‚ â”‚ â””â”€â”€ scan.js  
â”‚ â”œâ”€â”€ server.js  
â”‚ â”œâ”€â”€ services  
â”‚ â”‚ â”œâ”€â”€ analyzer.js  
â”‚ â”‚ â”œâ”€â”€ cookieAnalysis.js  
â”‚ â”‚ â”œâ”€â”€ crawler.js  
â”‚ â”‚ â”œâ”€â”€ ownershipGraph.js  
â”‚ â”‚ â””â”€â”€ scriptIntelligence.js  
â”‚ â””â”€â”€ worker.js  
â”œâ”€â”€ docker-compose.dev.yml  
â”œâ”€â”€ docker-compose.yml  
â”œâ”€â”€ frontend  
â”‚ â”œâ”€â”€ Dockerfile  
â”‚ â”œâ”€â”€ Dockerfile.dev  
â”‚ â”œâ”€â”€ index.html  
â”‚ â”œâ”€â”€ nginx.conf  
â”‚ â”œâ”€â”€ package.json  
â”‚ â”œâ”€â”€ package-lock.json  
â”‚ â”œâ”€â”€ src  
â”‚ â”‚ â”œâ”€â”€ App.jsx  
â”‚ â”‚ â”œâ”€â”€ components  
â”‚ â”‚ â”‚ â”œâ”€â”€ CookieAnalysis.jsx  
â”‚ â”‚ â”‚ â”œâ”€â”€ CrawlMeta.jsx  
â”‚ â”‚ â”‚ â”œâ”€â”€ DarkPatterns.jsx  
â”‚ â”‚ â”‚ â”œâ”€â”€ DomainCloud.jsx  
â”‚ â”‚ â”‚ â”œâ”€â”€ FingerprintReport.jsx  
â”‚ â”‚ â”‚ â”œâ”€â”€ OwnershipGraph.jsx  
â”‚ â”‚ â”‚ â”œâ”€â”€ ScoreMeter.jsx  
â”‚ â”‚ â”‚ â”œâ”€â”€ ScriptIntelligence.jsx  
â”‚ â”‚ â”‚ â”œâ”€â”€ SignalList.jsx  
â”‚ â”‚ â”‚ â””â”€â”€ TrackerList.jsx  
â”‚ â”‚ â”œâ”€â”€ lib  
â”‚ â”‚ â”‚ â””â”€â”€ api.js  
â”‚ â”‚ â”œâ”€â”€ main.jsx  
â”‚ â”‚ â””â”€â”€ styles.css  
â”‚ â””â”€â”€ vite.config.js  
â””â”€â”€ README.md
```

## Docker Setup (Recommended)

### Quick start

```bash
# 1. Clone repository
git clone https://github.com/TanvirTian/Lunaris
cd Lunaris

# 2. Create root environment file (Docker Compose configuration)
cp .env.example .env
# Set POSTGRES_PASSWORD to match backend/.env

# 3. Configure backend runtime environment
cp backend/.env.example backend/.env

# 4. Build and start all services
docker compose up -d --build

# 5. Verify services are running
docker compose ps

# 6. Test API health
curl http://localhost:8000/health

```

Open `http://localhost:3000`


### Common commands

```bash
# View all container statuses
docker compose ps

# Follow logs for a specific container
docker compose logs -f backend
docker compose logs -f worker
docker compose logs -f frontend

# Restart a single container (e.g. after config change)
docker compose restart backend

# Rebuild after source code changes
docker compose up -d --build

# Stop all containers (data is preserved in volumes)
docker compose down

# Stop and DELETE all data (volumes wiped)
docker compose down -v

# Open a shell inside a container
docker exec -it lunaris-backend sh
docker exec -it lunaris-postgres psql -U postgres -d privacy_analyzer

```

### Development mode (hot reload)

For active development â€” mounts your local source into containers, restarts Node on file changes:

```bash
docker compose -f docker-compose.yml -f docker-compose.dev.yml up

```

Add an alias to `~/.bashrc` for convenience:

```bash
alias dcdev="docker compose -f docker-compose.yml -f docker-compose.dev.yml"
# then just:
dcdev up

```

In dev mode:

-   Backend and worker use `node --watch` â€” restart automatically on `.js` file saves
-   Frontend uses Vite dev server with full HMR â€” browser updates without page reload
-   Source code is volume-mounted â€” no rebuild needed for code changes
-   Only rebuild when `package.json` or `Dockerfile` changes

### Data persistence

Both database volumes persist across restarts:

```yaml
volumes:
  postgres_data:   # all scan jobs, results, history
  redis_data:      # queue state snapshot (every 60s)

```

`docker compose down` keeps your data. `docker compose down -v` deletes it permanently.

## Local Development 

**Prerequisites:** Node.js 20+, PostgreSQL, Redis

```bash
# 1. Clone and install
git clone https://github.com/TanvirTian/Lunaris
cd lunaris

# 2. Backend
cd backend
npm install
cp .env.example .env
# Edit .env â€” set DATABASE_URL and REDIS_URL

# 3. Run database migrations
npx prisma migrate dev

# 4. Start backend + worker (two terminals)
npm start          # terminal 1 â€” API on http://localhost:8000
node worker.js     # terminal 2 â€” background worker

# 5. Frontend
cd ../frontend
npm install
npm run dev        # http://localhost:3000
```


## API Reference

### `POST /analyze`
Submit a URL for scanning.

```json
// Request
{ "url": "https://example.com" }

// Response 202
{
  "jobId": "uuid",
  "status": "PENDING",
  "pollUrl": "/scan/uuid"
}
```

### `GET /scan/:id`
Poll scan status and retrieve results.

```json
// Response (SUCCESS)
{
  "jobId": "uuid",
  "status": "SUCCESS",
  "result": {
    "score": 74,
    "riskLevel": "MODERATE",
    "summary": "...",
    "trackerCount": 3,
    "fingerprinting": { "canvas": false, "webgl": true },
    "data": { ... }
  }
}
```

### `GET /health`
```json
{ "status": "ok", "services": { "database": { "ok": true }, "redis": { "ok": true } } }
```

### `GET /metrics`
Returns queue depth, success/failure rates, crawl duration buckets, memory usage.

## Privacy Score Model

Score starts at **100**. Deductions are applied per signal:

| Signal | Deduction |
|---|---|
| Known tracker domain | âˆ’5 per tracker |
| Canvas / WebGL fingerprinting | âˆ’10 |
| Keylogger detected | âˆ’15 |
| Missing HTTPS | âˆ’20 |
| High-risk obfuscated scripts | âˆ’5 each |
| Dark pattern indicators | âˆ’5 each |

Final score is clamped to **0â€“100** and classified:

| Score | Risk Level |
|---|---|
| 80â€“100 | Low |
| 60â€“79 | Moderate |
| 40â€“59 | Elevated |
| 0â€“39 | High |


## Security

- **SSRF protection** â€” DNS pre-resolution, private IP range blocking (RFC1918, link-local, CGNAT), metadata endpoint blocking
- **Rate limiting** â€” 10 requests/minute per IP
- **Input validation** â€” structural URL parsing, no-dot hostname check, protocol allowlist
- **Non-root containers** â€” all Docker containers run as the `node` user
- **No secret baking** â€” environment variables only, never in image layers

## Scaling

The architecture supports horizontal scaling without code changes:

- **Multiple API servers** â€” stateless, add a load balancer in front
- **Multiple workers** â€” point additional `worker.js` instances at the same Redis and PostgreSQL. BullMQ's job locking ensures each job is processed exactly once
- **Database** â€” add PostgreSQL read replicas for analytics queries, PgBouncer for connection pooling at high concurrency


Built as a production system design study in asynchronous processing, browser automation, and privacy analysis.

